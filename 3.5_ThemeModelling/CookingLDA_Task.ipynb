{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment: \n",
    "## Готовим LDA по рецептам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как вы уже знаете, в тематическом моделировании делается предположение о том, что для определения тематики порядок слов в документе не важен; об этом гласит гипотеза «мешка слов». Сегодня мы будем работать с несколько нестандартной для тематического моделирования коллекцией, которую можно назвать «мешком ингредиентов», потому что на состоит из рецептов блюд разных кухонь. Тематические модели ищут слова, которые часто вместе встречаются в документах, и составляют из них темы. Мы попробуем применить эту идею к рецептам и найти кулинарные «темы». Эта коллекция хороша тем, что не требует предобработки. Кроме того, эта задача достаточно наглядно иллюстрирует принцип работы тематических моделей.\n",
    "\n",
    "Для выполнения заданий, помимо часто используемых в курсе библиотек, потребуются модули *json* и *gensim*. Первый входит в дистрибутив Anaconda, второй можно поставить командой \n",
    "\n",
    "*pip install gensim*\n",
    "\n",
    "Построение модели занимает некоторое время. На ноутбуке с процессором Intel Core i7 и тактовой частотой 2400 МГц на построение одной модели уходит менее 10 минут."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коллекция дана в json-формате: для каждого рецепта известны его id, кухня (cuisine) и список ингредиентов, в него входящих. Загрузить данные можно с помощью модуля json (он входит в дистрибутив Anaconda):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"recipes.json\") as f:\n",
    "    recipes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 10259, 'cuisine': 'greek', 'ingredients': ['romaine lettuce', 'black olives', 'grape tomatoes', 'garlic', 'pepper', 'purple onion', 'seasoning', 'garbanzo beans', 'feta cheese crumbles']}\n"
     ]
    }
   ],
   "source": [
    "print(recipes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Составление корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tater/ML_coursera/ML/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша коллекция небольшая, и целиком помещается в оперативную память. Gensim может работать с такими данными и не требует их сохранения на диск в специальном формате. Для этого коллекция должна быть представлена в виде списка списков, каждый внутренний список соответствует отдельному документу и состоит из его слов. Пример коллекции из двух документов: \n",
    "\n",
    "[[\"hello\", \"world\"], [\"programming\", \"in\", \"python\"]]\n",
    "\n",
    "Преобразуем наши данные в такой формат, а затем создадим объекты corpus и dictionary, с которыми будет работать модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [recipe[\"ingredients\"] for recipe in recipes]\n",
    "dictionary = corpora.Dictionary(texts)   # составляем словарь\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]  # составляем корпус документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['romaine lettuce', 'black olives', 'grape tomatoes', 'garlic', 'pepper', 'purple onion', 'seasoning', 'garbanzo beans', 'feta cheese crumbles']\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У объекта dictionary есть полезная переменная dictionary.token2id, позволяющая находить соответствие между ингредиентами и их индексами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели\n",
    "Вам может понадобиться [документация](https://radimrehurek.com/gensim/models/ldamodel.html) LDA в gensim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 1.__ Обучите модель LDA с 40 темами, установив количество проходов по коллекции 5 и оставив остальные параметры по умолчанию. \n",
    "\n",
    "\n",
    "Затем вызовите метод модели *show_topics*, указав количество тем 40 и количество токенов 10, и сохраните результат (топы ингредиентов в темах) в отдельную переменную. Если при вызове метода *show_topics* указать параметр *formatted=True*, то топы ингредиентов будет удобно выводить на печать, если *formatted=False*, будет удобно работать со списком программно. Выведите топы на печать, рассмотрите темы, а затем ответьте на вопрос:\n",
    "\n",
    "Сколько раз ингредиенты \"salt\", \"sugar\", \"water\", \"mushrooms\", \"chicken\", \"eggs\" встретились среди топов-10 всех 40 тем? При ответе __не нужно__ учитывать составные ингредиенты, например, \"hot water\".\n",
    "\n",
    "Передайте 6 чисел в функцию save_answers1 и загрузите сгенерированный файл в форму.\n",
    "\n",
    "У gensim нет возможности фиксировать случайное приближение через параметры метода, но библиотека использует numpy для инициализации матриц. Поэтому, по утверждению автора библиотеки, фиксировать случайное приближение нужно командой, которая написана в следующей ячейке. __Перед строкой кода с построением модели обязательно вставляйте указанную строку фиксации random.seed.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40 s, sys: 18.4 ms, total: 40 s\n",
      "Wall time: 40 s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(76543)\n",
    "# здесь код для построения модели:\n",
    "%time ldamodel = models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=40, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.099*\"chopped onion\" + 0.078*\"garlic cloves\" + 0.059*\"salt\" + 0.052*\"fat free less sodium chicken broth\" + 0.046*\"sliced green onions\" + 0.044*\"water\" + 0.042*\"cooking spray\" + 0.037*\"spinach\" + 0.036*\"ground red pepper\" + 0.036*\"green beans\"'),\n",
       " (1,\n",
       "  '0.101*\"corn kernels\" + 0.086*\"diced onions\" + 0.072*\"tortillas\" + 0.066*\"vegetable stock\" + 0.050*\"chopped fresh chives\" + 0.049*\"sliced black olives\" + 0.048*\"cream cheese, soften\" + 0.043*\"lard\" + 0.034*\"jack cheese\" + 0.024*\"canned black beans\"'),\n",
       " (2,\n",
       "  '0.091*\"olive oil\" + 0.061*\"crushed red pepper\" + 0.060*\"fresh parsley\" + 0.042*\"garlic cloves\" + 0.041*\"grated parmesan cheese\" + 0.040*\"butter\" + 0.037*\"cherry tomatoes\" + 0.035*\"salt\" + 0.035*\"low salt chicken broth\" + 0.033*\"fresh rosemary\"'),\n",
       " (3,\n",
       "  '0.075*\"bacon\" + 0.073*\"salt\" + 0.062*\"red pepper flakes\" + 0.054*\"onions\" + 0.051*\"garlic\" + 0.049*\"ground black pepper\" + 0.043*\"chicken thighs\" + 0.042*\"pasta\" + 0.034*\"olive oil\" + 0.028*\"mushrooms\"'),\n",
       " (4,\n",
       "  '0.093*\"cooking spray\" + 0.068*\"salt\" + 0.061*\"powdered sugar\" + 0.054*\"all-purpose flour\" + 0.050*\"large egg whites\" + 0.050*\"sugar\" + 0.049*\"large eggs\" + 0.044*\"cream cheese\" + 0.044*\"yellow corn meal\" + 0.041*\"butter\"'),\n",
       " (5,\n",
       "  '0.093*\"shallots\" + 0.092*\"large garlic cloves\" + 0.075*\"dry white wine\" + 0.057*\"olive oil\" + 0.045*\"finely chopped onion\" + 0.034*\"salt\" + 0.033*\"unsalted butter\" + 0.032*\"white wine vinegar\" + 0.030*\"arborio rice\" + 0.027*\"saffron threads\"'),\n",
       " (6,\n",
       "  '0.079*\"fresh thyme\" + 0.079*\"dry red wine\" + 0.053*\"pork tenderloin\" + 0.048*\"reduced sodium soy sauce\" + 0.047*\"cilantro sprigs\" + 0.041*\"beef broth\" + 0.037*\"peppercorns\" + 0.033*\"cremini mushrooms\" + 0.023*\"rosemary\" + 0.023*\"daikon\"'),\n",
       " (7,\n",
       "  '0.092*\"soy sauce\" + 0.052*\"sesame oil\" + 0.043*\"scallions\" + 0.042*\"green onions\" + 0.041*\"rice vinegar\" + 0.040*\"sugar\" + 0.036*\"corn starch\" + 0.035*\"garlic\" + 0.033*\"vegetable oil\" + 0.027*\"fresh ginger\"'),\n",
       " (8,\n",
       "  '0.174*\"garlic powder\" + 0.121*\"cayenne pepper\" + 0.065*\"onion powder\" + 0.047*\"ground black pepper\" + 0.043*\"smoked paprika\" + 0.040*\"black pepper\" + 0.040*\"pinenuts\" + 0.038*\"salt\" + 0.029*\"fresh spinach\" + 0.025*\"dried oregano\"'),\n",
       " (9,\n",
       "  '0.114*\"extra-virgin olive oil\" + 0.068*\"garlic cloves\" + 0.060*\"fresh lemon juice\" + 0.056*\"salt\" + 0.054*\"olive oil\" + 0.052*\"ground black pepper\" + 0.047*\"plum tomatoes\" + 0.043*\"purple onion\" + 0.035*\"balsamic vinegar\" + 0.034*\"fresh basil\"'),\n",
       " (10,\n",
       "  '0.065*\"broccoli florets\" + 0.049*\"button mushrooms\" + 0.048*\"crème fraîche\" + 0.046*\"yellow squash\" + 0.044*\"radishes\" + 0.038*\"greek style plain yogurt\" + 0.038*\"pork sausages\" + 0.035*\"watercress\" + 0.032*\"quickcooking grits\" + 0.031*\"ripe olives\"'),\n",
       " (11,\n",
       "  '0.138*\"lime\" + 0.083*\"lime juice\" + 0.057*\"fresh cilantro\" + 0.039*\"chopped cilantro\" + 0.039*\"purple onion\" + 0.031*\"mango\" + 0.030*\"garlic\" + 0.029*\"lime wedges\" + 0.028*\"jalapeno chilies\" + 0.023*\"salt\"'),\n",
       " (12,\n",
       "  '0.097*\"cheese\" + 0.093*\"ricotta cheese\" + 0.088*\"orange juice\" + 0.059*\"sliced mushrooms\" + 0.058*\"baby spinach\" + 0.053*\"vegetable broth\" + 0.052*\"vegetable oil cooking spray\" + 0.037*\"frozen chopped spinach\" + 0.035*\"part-skim mozzarella cheese\" + 0.034*\"italian sausage\"'),\n",
       " (13,\n",
       "  '0.099*\"diced tomatoes\" + 0.062*\"dried oregano\" + 0.061*\"onions\" + 0.052*\"tomato sauce\" + 0.047*\"garlic\" + 0.046*\"salt\" + 0.046*\"tomato paste\" + 0.039*\"olive oil\" + 0.033*\"crushed tomatoes\" + 0.029*\"ground beef\"'),\n",
       " (14,\n",
       "  '0.117*\"tomatoes\" + 0.091*\"salt\" + 0.083*\"red wine vinegar\" + 0.071*\"olive oil\" + 0.065*\"cucumber\" + 0.055*\"pepper\" + 0.052*\"lemon juice\" + 0.043*\"fresh oregano\" + 0.035*\"garlic\" + 0.034*\"purple onion\"'),\n",
       " (15,\n",
       "  '0.202*\"lemon\" + 0.055*\"orange\" + 0.053*\"boiling water\" + 0.049*\"fine sea salt\" + 0.045*\"sugar\" + 0.043*\"cold water\" + 0.039*\"fennel seeds\" + 0.032*\"almonds\" + 0.031*\"water\" + 0.028*\"mint\"'),\n",
       " (16,\n",
       "  '0.102*\"chopped cilantro fresh\" + 0.074*\"fresh lime juice\" + 0.069*\"jalapeno chilies\" + 0.051*\"white onion\" + 0.046*\"salt\" + 0.042*\"avocado\" + 0.038*\"garlic cloves\" + 0.037*\"ground cumin\" + 0.028*\"vegetable oil\" + 0.028*\"cilantro leaves\"'),\n",
       " (17,\n",
       "  '0.101*\"ground ginger\" + 0.100*\"ground cinnamon\" + 0.082*\"raisins\" + 0.077*\"ground cloves\" + 0.071*\"white wine\" + 0.061*\"ground allspice\" + 0.055*\"fresh mushrooms\" + 0.050*\"lean ground beef\" + 0.033*\"dried rosemary\" + 0.030*\"iceberg lettuce\"'),\n",
       " (18,\n",
       "  '0.118*\"parmesan cheese\" + 0.073*\"warm water\" + 0.064*\"salt\" + 0.053*\"olive oil\" + 0.048*\"dried basil\" + 0.039*\"grits\" + 0.033*\"kale\" + 0.031*\"plain flour\" + 0.029*\"water\" + 0.027*\"dry yeast\"'),\n",
       " (19,\n",
       "  '0.121*\"unsalted butter\" + 0.096*\"all-purpose flour\" + 0.094*\"large eggs\" + 0.076*\"salt\" + 0.062*\"sugar\" + 0.039*\"whole milk\" + 0.038*\"granulated sugar\" + 0.034*\"baking powder\" + 0.026*\"large egg yolks\" + 0.026*\"buttermilk\"'),\n",
       " (20,\n",
       "  '0.118*\"flat leaf parsley\" + 0.101*\"freshly ground pepper\" + 0.057*\"extra-virgin olive oil\" + 0.053*\"garlic cloves\" + 0.044*\"large shrimp\" + 0.042*\"salt\" + 0.042*\"olive oil\" + 0.033*\"dry bread crumbs\" + 0.027*\"ground black pepper\" + 0.025*\"kosher salt\"'),\n",
       " (21,\n",
       "  '0.148*\"chicken broth\" + 0.067*\"green bell pepper\" + 0.060*\"boneless skinless chicken breast halves\" + 0.056*\"boneless skinless chicken breasts\" + 0.056*\"onions\" + 0.050*\"chicken breasts\" + 0.043*\"red bell pepper\" + 0.040*\"butter\" + 0.040*\"pepper\" + 0.035*\"salt\"'),\n",
       " (22,\n",
       "  '0.114*\"grated parmesan cheese\" + 0.066*\"zucchini\" + 0.063*\"olive oil\" + 0.044*\"salt\" + 0.041*\"mozzarella cheese\" + 0.040*\"garlic\" + 0.038*\"shredded mozzarella cheese\" + 0.036*\"eggplant\" + 0.033*\"pepper\" + 0.032*\"eggs\"'),\n",
       " (23,\n",
       "  '0.078*\"brown sugar\" + 0.051*\"water\" + 0.050*\"salt\" + 0.048*\"soy sauce\" + 0.046*\"white pepper\" + 0.044*\"oil\" + 0.040*\"sugar\" + 0.038*\"sauce\" + 0.034*\"garlic\" + 0.034*\"ketchup\"'),\n",
       " (24,\n",
       "  '0.074*\"ground cumin\" + 0.047*\"salt\" + 0.047*\"ground coriander\" + 0.039*\"curry powder\" + 0.033*\"onions\" + 0.028*\"garlic\" + 0.028*\"vegetable oil\" + 0.027*\"ground turmeric\" + 0.026*\"garlic cloves\" + 0.024*\"fresh ginger\"'),\n",
       " (25,\n",
       "  '0.090*\"hot water\" + 0.083*\"chopped garlic\" + 0.065*\"peanut oil\" + 0.059*\"rice wine\" + 0.046*\"hot red pepper flakes\" + 0.037*\"corn oil\" + 0.033*\"fontina cheese\" + 0.031*\"marsala wine\" + 0.030*\"seasoning\" + 0.030*\"garlic chili sauce\"'),\n",
       " (26,\n",
       "  '0.082*\"mirin\" + 0.072*\"chickpeas\" + 0.050*\"red pepper\" + 0.050*\"mint leaves\" + 0.048*\"juice\" + 0.047*\"chopped fresh mint\" + 0.039*\"sugar\" + 0.036*\"fresh coriander\" + 0.035*\"sake\" + 0.030*\"grated lemon zest\"'),\n",
       " (27,\n",
       "  '0.184*\"heavy cream\" + 0.102*\"cheddar cheese\" + 0.068*\"frozen peas\" + 0.066*\"grated nutmeg\" + 0.042*\"bananas\" + 0.037*\"bread\" + 0.026*\"ice\" + 0.024*\"adobo sauce\" + 0.023*\"old bay seasoning\" + 0.022*\"butter\"'),\n",
       " (28,\n",
       "  '0.117*\"oil\" + 0.087*\"salt\" + 0.053*\"cilantro leaves\" + 0.052*\"green chilies\" + 0.048*\"cumin seed\" + 0.047*\"onions\" + 0.042*\"ground turmeric\" + 0.041*\"water\" + 0.033*\"chili powder\" + 0.030*\"tomatoes\"'),\n",
       " (29,\n",
       "  '0.077*\"sour cream\" + 0.047*\"salsa\" + 0.046*\"flour tortillas\" + 0.045*\"chili powder\" + 0.041*\"shredded cheddar cheese\" + 0.041*\"corn tortillas\" + 0.040*\"black beans\" + 0.035*\"cilantro\" + 0.030*\"salt\" + 0.029*\"ground cumin\"'),\n",
       " (30,\n",
       "  '0.092*\"sugar\" + 0.080*\"whipping cream\" + 0.076*\"egg yolks\" + 0.055*\"egg whites\" + 0.050*\"vanilla extract\" + 0.050*\"butter\" + 0.043*\"half & half\" + 0.037*\"sweetened condensed milk\" + 0.034*\"water\" + 0.031*\"strawberries\"'),\n",
       " (31,\n",
       "  '0.099*\"shrimp\" + 0.060*\"medium shrimp\" + 0.043*\"vegetable oil\" + 0.043*\"long-grain rice\" + 0.040*\"green onions\" + 0.034*\"long grain white rice\" + 0.033*\"rice noodles\" + 0.033*\"cajun seasoning\" + 0.029*\"hot pepper sauce\" + 0.029*\"scallions\"'),\n",
       " (32,\n",
       "  '0.112*\"eggs\" + 0.104*\"milk\" + 0.096*\"salt\" + 0.072*\"butter\" + 0.067*\"all-purpose flour\" + 0.042*\"flour\" + 0.041*\"baking powder\" + 0.041*\"white sugar\" + 0.040*\"sugar\" + 0.023*\"water\"'),\n",
       " (33,\n",
       "  '0.061*\"rice\" + 0.053*\"coriander\" + 0.049*\"onions\" + 0.047*\"salt\" + 0.042*\"garam masala\" + 0.041*\"tumeric\" + 0.040*\"ginger\" + 0.038*\"cabbage\" + 0.035*\"garlic\" + 0.033*\"ghee\"'),\n",
       " (34,\n",
       "  '0.092*\"cinnamon sticks\" + 0.085*\"clove\" + 0.072*\"black peppercorns\" + 0.042*\"chopped tomatoes\" + 0.042*\"cream\" + 0.039*\"garlic paste\" + 0.035*\"coriander seeds\" + 0.035*\"yoghurt\" + 0.033*\"fresh dill\" + 0.028*\"onions\"'),\n",
       " (35,\n",
       "  '0.068*\"onions\" + 0.052*\"bay leaves\" + 0.052*\"carrots\" + 0.050*\"celery\" + 0.050*\"salt\" + 0.045*\"bay leaf\" + 0.044*\"dried thyme\" + 0.043*\"water\" + 0.033*\"garlic\" + 0.030*\"ground black pepper\"'),\n",
       " (36,\n",
       "  '0.113*\"salt\" + 0.090*\"pepper\" + 0.074*\"paprika\" + 0.070*\"onions\" + 0.057*\"potatoes\" + 0.043*\"butter\" + 0.038*\"garlic\" + 0.035*\"olive oil\" + 0.028*\"worcestershire sauce\" + 0.023*\"parsley\"'),\n",
       " (37,\n",
       "  '0.110*\"sea salt\" + 0.066*\"coarse salt\" + 0.057*\"crushed red pepper flakes\" + 0.051*\"extra-virgin olive oil\" + 0.045*\"ground black pepper\" + 0.043*\"celery ribs\" + 0.041*\"ground pepper\" + 0.039*\"kosher salt\" + 0.034*\"parmigiano reggiano cheese\" + 0.033*\"garlic cloves\"'),\n",
       " (38,\n",
       "  '0.115*\"fish sauce\" + 0.056*\"coconut milk\" + 0.042*\"garlic\" + 0.038*\"red chili peppers\" + 0.037*\"shallots\" + 0.034*\"lemongrass\" + 0.028*\"sugar\" + 0.026*\"vegetable oil\" + 0.025*\"boneless chicken skinless thigh\" + 0.024*\"water\"'),\n",
       " (39,\n",
       "  '0.153*\"mayonaise\" + 0.095*\"dijon mustard\" + 0.069*\"cider vinegar\" + 0.069*\"cracked black pepper\" + 0.047*\"roma tomatoes\" + 0.043*\"white rice\" + 0.043*\"lemon wedge\" + 0.036*\"romaine lettuce\" + 0.025*\"green onions\" + 0.024*\"chicken wings\"')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = ldamodel.show_topics(num_topics=40, num_words=10, formatted=False)\n",
    "ldamodel.print_topics(num_topics=40, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24, 9, 9, 1, 0, 2]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = [term[0] for topic in topics for term in topic[1]]\n",
    "groups = Counter(terms)\n",
    "\n",
    "answer1 = [groups[k] for k in [\"salt\", \"sugar\", \"water\", \"mushrooms\", \"chicken\", \"eggs\"]]\n",
    "answer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_salt, c_sugar, c_water, c_mushrooms, c_chicken, c_eggs = answer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 9 9 1 0 2\n"
     ]
    }
   ],
   "source": [
    "print(c_salt, c_sugar, c_water, c_mushrooms, c_chicken, c_eggs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers1(c_salt, c_sugar, c_water, c_mushrooms, c_chicken, c_eggs):\n",
    "    with open(\"cooking_LDA_pa_task1.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(el) for el in [c_salt, c_sugar, c_water, c_mushrooms, c_chicken, c_eggs]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_answers1(c_salt, c_sugar, c_water, c_mushrooms, c_chicken, c_eggs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фильтрация словаря\n",
    "В топах тем гораздо чаще встречаются первые три рассмотренных ингредиента, чем последние три. При этом наличие в рецепте курицы, яиц и грибов яснее дает понять, что мы будем готовить, чем наличие соли, сахара и воды. Таким образом, даже в рецептах есть слова, часто встречающиеся в текстах и не несущие смысловой нагрузки, и поэтому их не желательно видеть в темах. Наиболее простой прием борьбы с такими фоновыми элементами — фильтрация словаря по частоте. Обычно словарь фильтруют с двух сторон: убирают очень редкие слова (в целях экономии памяти) и очень частые слова (в целях повышения интерпретируемости тем). Мы уберем только частые слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "dictionary2 = copy.deepcopy(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 2.__ У объекта dictionary2 есть переменная *dfs* — это словарь, ключами которого являются id токена, а элементами — число раз, сколько слово встретилось во всей коллекции. Сохраните в отдельный список ингредиенты, которые встретились в коллекции больше 4000 раз. Вызовите метод словаря *filter_tokens*, подав в качестве первого аргумента полученный список популярных ингредиентов. Вычислите две величины: dict_size_before и dict_size_after — размер словаря до и после фильтрации.\n",
    "\n",
    "Затем, используя новый словарь, создайте новый корпус документов, corpus2, по аналогии с тем, как это сделано в начале ноутбука. Вычислите две величины: corpus_size_before и corpus_size_after — суммарное количество ингредиентов в корпусе (для каждого документа вычислите число различных ингредиентов в нем и просуммируйте по всем документам) до и после фильтрации.\n",
    "\n",
    "Передайте величины dict_size_before, dict_size_after, corpus_size_before, corpus_size_after в функцию save_answers2 и загрузите сгенерированный файл в форму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 15, 11, 18, 20, 29, 44, 52, 59, 104, 114]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_size_before = len(dictionary2)\n",
    "frequent_words = [token_id for token_id, freq in dictionary2.dfs.items() if freq > 4000]\n",
    "frequent_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary2.filter_tokens(frequent_words)\n",
    "dict_size_after = len(dictionary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus2 = [dictionary2.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_size(corps):\n",
    "    x = 0\n",
    "    for item in corps:\n",
    "        x += len(item)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_size_before = corpus_size(corpus)\n",
    "corpus_size_after = corpus_size(corpus2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers2(dict_size_before, dict_size_after, corpus_size_before, corpus_size_after):\n",
    "    with open(\"cooking_LDA_pa_task2.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(el) for el in [dict_size_before, dict_size_after, corpus_size_before, corpus_size_after]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_answers2(dict_size_before, dict_size_after, corpus_size_before, corpus_size_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение когерентностей\n",
    "__Задание 3.__ Постройте еще одну модель по корпусу corpus2 и словарю dictionary2, остальные параметры оставьте такими же, как при первом построении модели. Сохраните новую модель в другую переменную (не перезаписывайте предыдущую модель). Не забудьте про фиксирование seed!\n",
    "\n",
    "Затем воспользуйтесь методом *top_topics* модели, чтобы вычислить ее когерентность. Передайте в качестве аргумента соответствующий модели корпус. Метод вернет список кортежей (топ токенов, когерентность), отсортированных по убыванию последней. Вычислите среднюю по всем темам когерентность для каждой из двух моделей и передайте в функцию save_answers3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.2 s, sys: 3.76 ms, total: 33.2 s\n",
      "Wall time: 33.2 s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(76543)\n",
    "%time ldamodel1 = models.ldamodel.LdaModel(corpus2, id2word=dictionary2, num_topics=40, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-37-e359a19eab1e>:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  coherence = np.array(top_topics)[:,1].mean()\n",
      "<ipython-input-37-e359a19eab1e>:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  coherence2 = np.array(top_topics2)[:,1].mean()\n"
     ]
    }
   ],
   "source": [
    "top_topics = ldamodel.top_topics(corpus)\n",
    "top_topics2 = ldamodel1.top_topics(corpus2)\n",
    "coherence = np.array(top_topics)[:,1].mean()\n",
    "coherence2 = np.array(top_topics2)[:,1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers3(coherence, coherence2):\n",
    "    with open(\"cooking_LDA_pa_task3.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([\"%3f\"%el for el in [coherence, coherence2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_answers3(coherence, coherence2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считается, что когерентность хорошо соотносится с человеческими оценками интерпретируемости тем. Поэтому на больших текстовых коллекциях когерентность обычно повышается, если убрать фоновую лексику. Однако в нашем случае этого не произошло. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изучение влияния гиперпараметра alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом разделе мы будем работать со второй моделью, то есть той, которая построена по сокращенному корпусу. \n",
    "\n",
    "Пока что мы посмотрели только на матрицу темы-слова, теперь давайте посмотрим на матрицу темы-документы. Выведите темы для нулевого (или любого другого) документа из корпуса, воспользовавшись методом *get_document_topics* второй модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(25, 0.12812185), (31, 0.6175929), (33, 0.13865705)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel1.get_document_topics(corpus2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также выведите содержимое переменной *.alpha* второй модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n",
       "       0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n",
       "       0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n",
       "       0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n",
       "       0.025, 0.025, 0.025, 0.025], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel1.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У вас должно получиться, что документ характеризуется небольшим числом тем. Попробуем поменять гиперпараметр alpha, задающий априорное распределение Дирихле для распределений тем в документах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 4.__ Обучите третью модель: используйте сокращенный корпус (corpus2 и dictionary2) и установите параметр __alpha=1__, passes=5. Не забудьте про фиксацию seed! Выведите темы новой модели для нулевого документа; должно получиться, что распределение над множеством тем практически равномерное. Чтобы убедиться в том, что во второй модели документы описываются гораздо более разреженными распределениями, чем в третьей, посчитайте суммарное количество элементов, __превосходящих 0.01__, в матрицах темы-документы обеих моделей. Другими словами, запросите темы  модели для каждого документа с параметром *minimum_probability=0.01* и просуммируйте число элементов в получаемых массивах. Передайте две суммы (сначала для модели с alpha по умолчанию, затем для модели в alpha=1) в функцию save_answers4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(76543)\n",
    "ldamodel3 = models.ldamodel.LdaModel(corpus2, id2word=dictionary2, num_topics=40, passes=5, alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.021397676),\n",
       " (1, 0.021295447),\n",
       " (2, 0.021276837),\n",
       " (3, 0.021365928),\n",
       " (4, 0.021295367),\n",
       " (5, 0.021311186),\n",
       " (6, 0.021304969),\n",
       " (7, 0.021280425),\n",
       " (8, 0.021401428),\n",
       " (9, 0.021379557),\n",
       " (10, 0.021837873),\n",
       " (11, 0.021492522),\n",
       " (12, 0.021276837),\n",
       " (13, 0.022189546),\n",
       " (14, 0.021718102),\n",
       " (15, 0.0215066),\n",
       " (16, 0.021404233),\n",
       " (17, 0.021964395),\n",
       " (18, 0.021329327),\n",
       " (19, 0.021678489),\n",
       " (20, 0.02465409),\n",
       " (21, 0.021277266),\n",
       " (22, 0.021276837),\n",
       " (23, 0.021284843),\n",
       " (24, 0.021771777),\n",
       " (25, 0.02149457),\n",
       " (26, 0.021462495),\n",
       " (27, 0.021634081),\n",
       " (28, 0.021495195),\n",
       " (29, 0.02130315),\n",
       " (30, 0.042615004),\n",
       " (31, 0.09219327),\n",
       " (32, 0.02150039),\n",
       " (33, 0.021278715),\n",
       " (34, 0.021446656),\n",
       " (35, 0.021365926),\n",
       " (36, 0.02133184),\n",
       " (37, 0.02128943),\n",
       " (38, 0.021277951),\n",
       " (39, 0.06833979)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel3.get_document_topics(corpus2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203680 1590960\n"
     ]
    }
   ],
   "source": [
    "def count_topics(model, corpuses):\n",
    "    accum = 0\n",
    "    for doc in corpuses:\n",
    "        topics = model.get_document_topics(doc, minimum_probability=0.01)\n",
    "        accum += len(topics)\n",
    "    return accum\n",
    "\n",
    "count_model2 = count_topics(ldamodel1, corpus2)\n",
    "count_model3 = count_topics(ldamodel3, corpus2)\n",
    "print(count_model2, count_model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers4(count_model2, count_model3):\n",
    "    with open(\"cooking_LDA_pa_task4.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(el) for el in [count_model2, count_model3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_answers4(count_model2, count_model3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, гиперпараметр __alpha__ влияет на разреженность распределений тем в документах. Аналогично гиперпараметр __eta__ влияет на разреженность распределений слов в темах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA как способ понижения размерности\n",
    "Иногда, распределения над темами, найденные с помощью LDA, добавляют в матрицу объекты-признаки как дополнительные, семантические, признаки, и это может улучшить качество решения задачи. Для простоты давайте просто обучим классификатор рецептов на кухни на признаках, полученных из LDA, и измерим точность (accuracy).\n",
    "\n",
    "__Задание 5.__ Используйте модель, построенную по сокращенной выборке с alpha по умолчанию (вторую модель). Составьте матрицу $\\Theta = p(t|d)$ вероятностей тем в документах; вы можете использовать тот же метод get_document_topics, а также вектор правильных ответов y (в том же порядке, в котором рецепты идут в переменной recipes). Создайте объект RandomForestClassifier со 100 деревьями, с помощью функции cross_val_score вычислите среднюю accuracy по трем фолдам (перемешивать данные не нужно) и передайте в функцию save_answers5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix(model, corpus):\n",
    "    matrix = {}\n",
    "    for i, doc in enumerate(corpus):\n",
    "        matrix[i] = {}\n",
    "        topics = model.get_document_topics(doc)\n",
    "        for topic in topics:\n",
    "            matrix[i][topic[0]] = topic[1]\n",
    "   \n",
    "    return matrix\n",
    "\n",
    "matrix = build_matrix(ldamodel1, corpus2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39774, 40), (39774,))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame.from_dict(matrix).fillna(0).T\n",
    "y = np.array([recipe['cuisine'] for recipe in recipes])\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers5(accuracy):\n",
    "     with open(\"cooking_LDA_pa_task5.txt\", \"w\") as fout:\n",
    "        fout.write(str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_answers5(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для такого большого количества классов это неплохая точность. Вы можете попроовать обучать RandomForest на исходной матрице частот слов, имеющей значительно большую размерность, и увидеть, что accuracy увеличивается на 10–15%. Таким образом, LDA собрал не всю, но достаточно большую часть информации из выборки, в матрице низкого ранга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA — вероятностная модель\n",
    "Матричное разложение, использующееся в LDA, интерпретируется как следующий процесс генерации документов.\n",
    "\n",
    "Для документа $d$ длины $n_d$:\n",
    "1. Из априорного распределения Дирихле с параметром alpha сгенерировать распределение над множеством тем: $\\theta_d \\sim Dirichlet(\\alpha)$\n",
    "1. Для каждого слова $w = 1, \\dots, n_d$:\n",
    "    1. Сгенерировать тему из дискретного распределения $t \\sim \\theta_{d}$\n",
    "    1. Сгенерировать слово из дискретного распределения $w \\sim \\phi_{t}$.\n",
    "    \n",
    "Подробнее об этом в [Википедии](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation).\n",
    "\n",
    "В контексте нашей задачи получается, что, используя данный генеративный процесс, можно создавать новые рецепты. Вы можете передать в функцию модель и число ингредиентов и сгенерировать рецепт :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recipe(model, num_ingredients):\n",
    "    theta = np.random.dirichlet(model.alpha)\n",
    "    for i in range(num_ingredients):\n",
    "        t = np.random.choice(np.arange(model.num_topics), p=theta)\n",
    "        topic = model.show_topic(t, topn=model.num_terms)\n",
    "        topic_distr = [x[1] for x in topic]\n",
    "        terms = [x[0] for x in topic]\n",
    "        w = np.random.choice(terms, p=topic_distr)\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Интерпретация построенной модели\n",
    "Вы можете рассмотреть топы ингредиентов каждой темы. Большиснтво тем сами по себе похожи на рецепты; в некоторых собираются продукты одного вида, например, свежие фрукты или разные виды сыра.\n",
    "\n",
    "Попробуем эмпирически соотнести наши темы с национальными кухнями (cuisine). Построим матрицу $A$ размера темы $x$ кухни, ее элементы $a_{tc}$ — суммы $p(t|d)$ по всем документам $d$, которые отнесены к кухне $c$. Нормируем матрицу на частоты рецептов по разным кухням, чтобы избежать дисбаланса между кухнями. Следующая функция получает на вход объект модели, объект корпуса и исходные данные и возвращает нормированную матрицу $A$. Ее удобно визуализировать с помощью seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import seaborn\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_topic_cuisine_matrix(model, corpus, recipes):\n",
    "    # составляем вектор целевых признаков\n",
    "    targets = list(set([recipe[\"cuisine\"] for recipe in recipes]))\n",
    "    # составляем матрицу\n",
    "    tc_matrix = pandas.DataFrame(data=np.zeros((model.num_topics, len(targets))), columns=targets)\n",
    "    for recipe, bow in zip(recipes, corpus):\n",
    "        recipe_topic = model.get_document_topics(bow)\n",
    "        for t, prob in recipe_topic:\n",
    "            tc_matrix[recipe[\"cuisine\"]][t] += prob\n",
    "    # нормируем матрицу\n",
    "    target_sums = pandas.DataFrame(data=np.zeros((1, len(targets))), columns=targets)\n",
    "    for recipe in recipes:\n",
    "        target_sums[recipe[\"cuisine\"]] += 1\n",
    "    return pandas.DataFrame(tc_matrix.values/target_sums.values, columns=tc_matrix.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matrix(tc_matrix):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    seaborn.heatmap(tc_matrix, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализируйте матрицу\n",
    "plot_matrix(compute_topic_cuisine_matrix(ldamodel, corpus, recipes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чем темнее квадрат в матрице, тем больше связь этой темы с данной кухней. Мы видим, что у нас есть темы, которые связаны с несколькими кухнями. Такие темы показывают набор ингредиентов, которые популярны в кухнях нескольких народов, то есть указывают на схожесть кухонь этих народов. Некоторые темы распределены по всем кухням равномерно, они показывают наборы продуктов, которые часто используются в кулинарии всех стран. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Жаль, что в датасете нет названий рецептов, иначе темы было бы проще интерпретировать..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заключение\n",
    "В этом задании вы построили несколько моделей LDA, посмотрели, на что влияют гиперпараметры модели и как можно использовать построенную модель. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
